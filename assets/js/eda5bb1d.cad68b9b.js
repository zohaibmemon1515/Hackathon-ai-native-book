"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[7981],{8396:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"vla-integration/chapter2/lesson1","title":"Large Language Models (LLMs) and Robotics","description":"Learning Objectives","source":"@site/docs/vla-integration/chapter2/lesson1.md","sourceDirName":"vla-integration/chapter2","slug":"/vla-integration/chapter2/lesson1","permalink":"/Hackathon-ai-native-book/docs/vla-integration/chapter2/lesson1","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla-integration/chapter2/lesson1.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Large Language Models (LLMs) and Robotics","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Language Models for Robot Control","permalink":"/Hackathon-ai-native-book/docs/category/language-models-for-robot-control"},"next":{"title":"Grounding Language to Robot Actions and Perceptions","permalink":"/Hackathon-ai-native-book/docs/vla-integration/chapter2/lesson2"}}');var i=t(4848),a=t(8453);const r={title:"Large Language Models (LLMs) and Robotics",sidebar_position:1},s=void 0,l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Concept Explanation",id:"concept-explanation",level:2},{value:"Real-World Examples",id:"real-world-examples",level:2},{value:"Mathematical or technical breakdown",id:"mathematical-or-technical-breakdown",level:2},{value:"Mini-Project",id:"mini-project",level:2},{value:"Summary",id:"summary",level:2},{value:"APA-style references",id:"apa-style-references",level:2}];function d(e){const n={code:"code",em:"em",h2:"h2",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understand the basic principles of Large Language Models (LLMs)."}),"\n",(0,i.jsx)(n.li,{children:"Explore how LLMs can be adapted for robotic control."}),"\n",(0,i.jsx)(n.li,{children:"Identify challenges and opportunities of using LLMs in robotics."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"concept-explanation",children:"Concept Explanation"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Large Language Models (LLMs)"})," are a class of artificial intelligence models that have been trained on vast amounts of text data, enabling them to understand, generate, and process human language with remarkable fluency. Their ability to grasp context, reason symbolically, and generate coherent responses makes them powerful tools for enhancing robotic capabilities beyond traditional hard-coded control."]}),"\n",(0,i.jsx)(n.p,{children:"How LLMs are adapted for robotics:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"High-level Planning"}),": Translating natural language goals into a sequence of executable robot actions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Code Generation"}),": Generating robot code or scripts based on verbal descriptions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error Recovery"}),": Understanding failure scenarios and suggesting recovery strategies in natural language."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Human-Robot Interaction"}),": Facilitating intuitive communication and instruction following."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"real-world-examples",children:"Real-World Examples"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'A robot receiving the command "make coffee" and the LLM expanding this into sub-goals: "find coffee machine", "insert pod", "press brew button".'}),"\n",(0,i.jsx)(n.li,{children:"An industrial robot that can be retrained for a new task by simply describing the new assembly process in natural language."}),"\n",(0,i.jsx)(n.li,{children:"A robot querying a human operator in natural language when encountering an ambiguous situation."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"mathematical-or-technical-breakdown",children:"Mathematical or technical breakdown"}),"\n",(0,i.jsxs)(n.p,{children:["LLMs typically use the ",(0,i.jsx)(n.strong,{children:"Transformer architecture"}),', which relies on self-attention mechanisms to weigh the importance of different words in an input sequence. When applied to robotics, LLMs often don\'t directly control motors. Instead, they act as a "reasoning engine" or a high-level planner:']}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Natural Language Input"}),': Human command (e.g., "pick up the red cube").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLM Processing"}),": The LLM interprets the command, potentially drawing on its knowledge base and context."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Plan Generation"}),": The LLM outputs a symbolic action plan or high-level function calls (e.g., ",(0,i.jsx)(n.code,{children:'pick_object("red cube")'}),")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robot Execution"}),": A lower-level robot control system executes these symbolic actions."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Challenges include grounding abstract language to physical reality, managing safety, and dealing with the inherent stochasticity of LLM outputs."}),"\n",(0,i.jsx)(n.h2,{id:"mini-project",children:"Mini-Project"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Design a Prompt for a Robot LLM (Conceptual)"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Imagine you are instructing a mobile robot to clean a room."}),"\n",(0,i.jsxs)(n.li,{children:["Design a natural language prompt that would enable an LLM to generate a sequence of high-level actions for the robot. Consider:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"What information should be included in the prompt (e.g., robot capabilities, objects in the room)?"}),"\n",(0,i.jsx)(n.li,{children:"What kind of output would you expect from the LLM (e.g., a list of steps, Python code snippets)?"}),"\n",(0,i.jsx)(n.li,{children:'How would you handle potential ambiguities or constraints (e.g., "don\'t touch the vase")?'}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"LLMs are transforming robotics by enabling more natural human-robot interaction and high-level task planning. While challenges remain in grounding language to physical reality and ensuring safety, their ability to understand and generate human-like text offers immense potential for more intuitive and adaptable robotic systems."}),"\n",(0,i.jsx)(n.h2,{id:"apa-style-references",children:"APA-style references"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Huang, K., et al. (2022). Do As I Say, Not As I Do: Explaining and Correcting Robot Actions in Natural Language. ",(0,i.jsx)(n.em,{children:"Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Shridhar, M., et al. (2023). Perceiver-Actor: A Multi-Tasking Visuomotor Policy for Embodied Agents. ",(0,i.jsx)(n.em,{children:"Conference on Robot Learning (CoRL)"}),"."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var o=t(6540);const i={},a=o.createContext(i);function r(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);